{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa3be4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional,Flatten, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10190c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a5f3bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e7961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0813b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null = df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af254f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweet\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d48898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59184298",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"Tweet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e463cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Tweet in range(len(df[\"Tweet\"])):\n",
    "    df[\"Tweet\"][Tweet]=re.sub(r'<[^<>]+>', repl=\" \",string=df[\"Tweet\"][Tweet]) #remove html tags\n",
    "    df[\"Tweet\"][Tweet]=re.sub(r'[^a-zA-Z0-9\\s]', repl=\" \",string=df[\"Tweet\"][Tweet]) #remove special characters/whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a637dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweet\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()\n",
    "def stemming(content):\n",
    "    #replace any non-alphabetic characters in the content variable with a space character\n",
    "    stemmed_content= re.sub('[^a-zA-Z]',' ',content)\n",
    "    #Convert all words into lower case letters\n",
    "    stemmed_content = stemmed_content.lower() \n",
    "    # Split the words into list\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    #generate a list of stemmed words from stemmed_content, excluding any stop words from the list\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    #Join the elements from the list 'stemmed_content' into a single string separated by spaces\n",
    "    stemmed_content = \" \".join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweet\"]= df[\"Tweet\"].apply(stemming)\n",
    "df[\"Tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)  # unique words limit set to 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee49a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(df['Tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ef01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding so that all reviews will be of length 500\n",
    "X = pad_sequences(X,maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9efb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d249d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized and stemmed sequences back to text format\n",
    "documents = []\n",
    "for sequence in X:\n",
    "    text = \" \".join([str(token) for token in sequence if token != 0])\n",
    "    documents.append(text)\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute TF-IDF scores on the entire dataset\n",
    "x = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc6b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c597a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6031df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open('tokenizer.pickle',\"wb\")\n",
    "pickle.dump(tokenizer,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25db631",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.get_dummies(Y_train)\n",
    "y_test=pd.get_dummies(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1 # +1 is necessary for embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee308a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724135f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a993f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41957525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Reshape, Bidirectional, LSTM\n",
    "\n",
    "\n",
    "# Convert sparse matrix to dense NumPy array\n",
    "X_train_dense = X_train.toarray()\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "y_train_np = y_train.to_numpy()\n",
    "\n",
    "\n",
    "# Define the model architecture with the correct output shape for multi-class classification\n",
    "num_classes = 4  # Number of actual number of classes in the data\n",
    "timesteps = 64   # Set the desired number of timesteps\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_dense.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Reshape((timesteps, -1)),  # Reshape the output of the previous Dense layer to (None, timesteps, features)\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(num_classes, activation='softmax')  # Softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model with categorical_crossentropy loss for multi-class classification\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8)\n",
    "# fitting the model with the updated architecture\n",
    "modelTraining = model.fit(X_train_dense, y_train_np,\n",
    "                          batch_size=64,\n",
    "                          epochs=15,\n",
    "                          validation_data=(X_train_dense, y_train_np), callbacks=[earlyStopping])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5246fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_train_dense, y_train_np, verbose=0)\n",
    "\n",
    "print(\"Test_accuracy = \", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing epoch values in variables\n",
    "epochs = range(1, len(modelTraining.history['accuracy']) + 1)\n",
    "accuracy = modelTraining.history['accuracy']\n",
    "loss = modelTraining.history['loss']\n",
    "val_accuracy = modelTraining.history['val_accuracy']\n",
    "val_loss = modelTraining.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40372a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, accuracy, 'r', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93990e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
